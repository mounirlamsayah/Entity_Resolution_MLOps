# PIPELINE DEFINITION
# Name: entity-matching-retrain-pipeline
# Description: Pipeline de re-entraînement avec nouvelles données
# Inputs:
#    base_model_path: str
#    batch_size: int [Default: 32.0]
#    epochs: int [Default: 5.0]
#    learning_rate: float [Default: 0.0001]
#    min_accuracy: float [Default: 0.75]
#    min_f1: float [Default: 0.75]
# Outputs:
#    data-preprocessing-component-output_metrics: system.Metrics
#    model-evaluation-component-output_metrics: system.Metrics
#    model-training-component-output_metrics: system.Metrics
components:
  comp-data-preprocessing-component:
    executorLabel: exec-data-preprocessing-component
    outputDefinitions:
      artifacts:
        output_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        num_positives:
          parameterType: NUMBER_INTEGER
        num_samples:
          parameterType: NUMBER_INTEGER
  comp-model-evaluation-component:
    executorLabel: exec-model-evaluation-component
    inputDefinitions:
      artifacts:
        input_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        input_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        accuracy:
          parameterType: NUMBER_DOUBLE
        f1_score:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        test_accuracy:
          parameterType: NUMBER_DOUBLE
        test_f1:
          parameterType: NUMBER_DOUBLE
  comp-model-training-component:
    executorLabel: exec-model-training-component
    inputDefinitions:
      artifacts:
        input_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_size:
          defaultValue: 32.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        epochs:
          defaultValue: 10.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        learning_rate:
          defaultValue: 0.001
          isOptional: true
          parameterType: NUMBER_DOUBLE
        num_positives:
          parameterType: NUMBER_INTEGER
        num_samples:
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        accuracy:
          parameterType: NUMBER_DOUBLE
        f1_score:
          parameterType: NUMBER_DOUBLE
  comp-model-validation-component:
    executorLabel: exec-model-validation-component
    inputDefinitions:
      parameters:
        min_accuracy:
          defaultValue: 0.7
          isOptional: true
          parameterType: NUMBER_DOUBLE
        min_f1:
          defaultValue: 0.7
          isOptional: true
          parameterType: NUMBER_DOUBLE
        test_accuracy:
          parameterType: NUMBER_DOUBLE
        test_f1:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
defaultPipelineRoot: gs://your-bucket/entity-matching-retrain
deploymentSpec:
  executors:
    exec-data-preprocessing-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_preprocessing_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_preprocessing_component(\n    output_dataset: Output[Dataset],\n\
          \    output_metrics: Output[Metrics]\n) -> NamedTuple('Outputs', [('num_samples',\
          \ int), ('num_positives', int)]):\n    \"\"\"Composant de preprocessing\
          \ des donn\xE9es\"\"\"\n\n    import subprocess\n    import json\n    import\
          \ os\n\n    # Ex\xE9cuter le preprocessing\n    result = subprocess.run([\n\
          \        'python', '/app/src/data_preprocessing.py'\n    ], capture_output=True,\
          \ text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"\
          Preprocessing failed: {result.stderr}\")\n\n    # Lire les m\xE9triques\
          \ du preprocessing\n    if os.path.exists('/app/models/processed_dataset.csv'):\n\
          \        import pandas as pd\n        df = pd.read_csv('/app/models/processed_dataset.csv')\n\
          \        num_samples = len(df)\n        num_positives = sum(df['label'])\n\
          \n        # Sauvegarder le dataset\n        df.to_csv(output_dataset.path,\
          \ index=False)\n\n        # Sauvegarder les m\xE9triques\n        metrics\
          \ = {\n            'num_samples': num_samples,\n            'num_positives':\
          \ num_positives,\n            'num_negatives': num_samples - num_positives,\n\
          \            'balance_ratio': num_positives / num_samples\n        }\n\n\
          \        with open(output_metrics.path, 'w') as f:\n            json.dump(metrics,\
          \ f)\n\n        return (num_samples, num_positives)\n    else:\n       \
          \ raise RuntimeError(\"Dataset processed non trouv\xE9\")\n\n"
        image: entity-matcher:latest
    exec-model-evaluation-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_evaluation_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_evaluation_component(\n    input_model: Input[Model],\n\
          \    input_dataset: Input[Dataset],\n    output_metrics: Output[Metrics],\n\
          \    accuracy: float,\n    f1_score: float\n) -> NamedTuple('Outputs', [('test_accuracy',\
          \ float), ('test_f1', float)]):\n    \"\"\"Composant d'\xE9valuation du\
          \ mod\xE8le\"\"\"\n\n    import json\n    import os\n    import shutil\n\
          \    import sys\n    import pandas as pd\n    import numpy as np\n    from\
          \ sklearn.metrics import accuracy_score, precision_score, recall_score,\
          \ f1_score as f1_metric\n\n    print(f\"\xC9valuation du mod\xE8le (accuracy\
          \ train: {accuracy:.4f}, f1 train: {f1_score:.4f})\")\n\n    # Cr\xE9er\
          \ les r\xE9pertoires n\xE9cessaires\n    os.makedirs('/app/models', exist_ok=True)\n\
          \n    # Copier le mod\xE8le d'entr\xE9e vers le r\xE9pertoire models\n \
          \   shutil.copy(input_model.path, '/app/models/siamese_entity_matcher.h5')\n\
          \n    try:\n        # Importer les classes n\xE9cessaires\n        sys.path.append('/app/src')\n\
          \        from app import EntityMatcher\n\n        # Initialiser le matcher\n\
          \        matcher = EntityMatcher()\n        success = matcher.load_model_and_tokenizer()\n\
          \n        if not success:\n            raise RuntimeError(\"\xC9chec du\
          \ chargement du mod\xE8le pour l'\xE9valuation\")\n\n        # Charger les\
          \ donn\xE9es de test\n        if os.path.exists('/app/models/X1_test.npy'):\n\
          \            X1_test = np.load('/app/models/X1_test.npy')\n            X2_test\
          \ = np.load('/app/models/X2_test.npy')\n            y_test = np.load('/app/models/y_test.npy')\n\
          \n            # Faire des pr\xE9dictions\n            predictions = []\n\
          \            batch_size = 32\n\n            for i in range(0, len(X1_test),\
          \ batch_size):\n                batch_X1 = X1_test[i:i+batch_size]\n   \
          \             batch_X2 = X2_test[i:i+batch_size]\n\n                batch_pred\
          \ = matcher.model.predict([batch_X1, batch_X2], verbose=0)\n           \
          \     predictions.extend(batch_pred.flatten())\n\n            # Convertir\
          \ en labels binaires\n            y_pred = (np.array(predictions) > 0.5).astype(int)\n\
          \n            # Calculer les m\xE9triques\n            test_accuracy = accuracy_score(y_test,\
          \ y_pred)\n            test_precision = precision_score(y_test, y_pred)\n\
          \            test_recall = recall_score(y_test, y_pred)\n            test_f1\
          \ = f1_metric(y_test, y_pred)\n\n            eval_results = {\n        \
          \        'accuracy': float(test_accuracy),\n                'precision':\
          \ float(test_precision),\n                'recall': float(test_recall),\n\
          \                'f1_score': float(test_f1),\n                'num_samples':\
          \ len(y_test)\n            }\n        else:\n            # Si pas de donn\xE9\
          es de test numpy, utiliser le CSV\n            dataset_df = pd.read_csv(input_dataset.path)\n\
          \            test_df = dataset_df.sample(n=min(1000, len(dataset_df)), random_state=42)\n\
          \n            predictions = []\n            true_labels = []\n\n       \
          \     for idx, row in test_df.iterrows():\n                text1 = f\"{row.get('nom_prenom_rs_clean_src',\
          \ '')} {row.get('adresse_clean_src', '')} {row.get('num_cin_clean_src',\
          \ '')}\"\n                text2 = f\"{row.get('nom_prenom_rs_clean_ref',\
          \ '')} {row.get('adresse_clean_ref', '')} {row.get('num_cin_clean_ref',\
          \ '')}\"\n\n                similarity = matcher.predict_similarity(text1,\
          \ text2)\n                if similarity is not None:\n                 \
          \   predictions.append(1 if similarity > 0.5 else 0)\n                 \
          \   true_labels.append(row['label'])\n\n            if predictions:\n  \
          \              test_accuracy = accuracy_score(true_labels, predictions)\n\
          \                test_precision = precision_score(true_labels, predictions)\n\
          \                test_recall = recall_score(true_labels, predictions)\n\
          \                test_f1 = f1_metric(true_labels, predictions)\n\n     \
          \           eval_results = {\n                    'accuracy': float(test_accuracy),\n\
          \                    'precision': float(test_precision),\n             \
          \       'recall': float(test_recall),\n                    'f1_score': float(test_f1),\n\
          \                    'num_samples': len(predictions)\n                }\n\
          \            else:\n                eval_results = {\n                 \
          \   'accuracy': accuracy,\n                    'f1_score': f1_score,\n \
          \                   'precision': 0.0,\n                    'recall': 0.0,\n\
          \                    'num_samples': 0\n                }\n\n        # Sauvegarder\
          \ les r\xE9sultats\n        with open(output_metrics.path, 'w') as f:\n\
          \            json.dump(eval_results, f)\n\n        with open('/app/models/evaluation_results.json',\
          \ 'w') as f:\n            json.dump(eval_results, f)\n\n        return (eval_results['accuracy'],\
          \ eval_results['f1_score'])\n\n    except Exception as e:\n        print(f\"\
          Erreur lors de l'\xE9valuation: {e}\")\n        # Retourner les m\xE9triques\
          \ d'entra\xEEnement en cas d'erreur\n        mock_results = {\n        \
          \    'accuracy': accuracy,\n            'f1_score': f1_score,\n        \
          \    'precision': 0.0,\n            'recall': 0.0,\n            'num_samples':\
          \ 0\n        }\n\n        with open(output_metrics.path, 'w') as f:\n  \
          \          json.dump(mock_results, f)\n\n        return (accuracy, f1_score)\n\
          \n"
        image: entity-matcher:latest
    exec-model-training-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_training_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_training_component(\n    input_dataset: Input[Dataset],\n\
          \    output_model: Output[Model],\n    output_metrics: Output[Metrics],\n\
          \    num_samples: int,\n    num_positives: int,\n    epochs: int = 10,\n\
          \    batch_size: int = 32,\n    learning_rate: float = 0.001\n) -> NamedTuple('Outputs',\
          \ [('accuracy', float), ('f1_score', float)]):\n    \"\"\"Composant d'entra\xEE\
          nement du mod\xE8le\"\"\"\n\n    import subprocess\n    import json\n  \
          \  import os\n    import shutil\n    import sys\n\n    print(f\"Entra\xEE\
          nement avec {num_samples} \xE9chantillons ({num_positives} positifs)\")\n\
          \    print(f\"Param\xE8tres: epochs={epochs}, batch_size={batch_size}, lr={learning_rate}\"\
          )\n\n    # Configurer les variables d'environnement pour l'entra\xEEnement\n\
          \    env = os.environ.copy()\n    env['EPOCHS'] = str(epochs)\n    env['BATCH_SIZE']\
          \ = str(batch_size)\n    env['LEARNING_RATE'] = str(learning_rate)\n\n \
          \   # Ex\xE9cuter l'entra\xEEnement\n    result = subprocess.run([\n   \
          \     sys.executable, '/app/src/model_training.py'\n    ], capture_output=True,\
          \ text=True, env=env)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"\
          Training failed: {result.stderr}\")\n\n    # V\xE9rifier que le mod\xE8\
          le a \xE9t\xE9 cr\xE9\xE9\n    model_path = '/app/models/siamese_entity_matcher.h5'\n\
          \    metrics_path = '/app/models/training_metrics.json'\n\n    if not os.path.exists(model_path):\n\
          \        raise RuntimeError(\"Mod\xE8le non g\xE9n\xE9r\xE9\")\n\n    #\
          \ Copier le mod\xE8le vers la sortie\n    shutil.copy(model_path, output_model.path)\n\
          \n    # Lire et copier les m\xE9triques\n    if os.path.exists(metrics_path):\n\
          \        with open(metrics_path, 'r') as f:\n            training_metrics\
          \ = json.load(f)\n\n        with open(output_metrics.path, 'w') as f:\n\
          \            json.dump(training_metrics, f)\n\n        accuracy = training_metrics.get('final_val_accuracy',\
          \ 0.0)\n        # Calculer F1 approximatif depuis precision/recall si disponible\n\
          \        precision = training_metrics.get('final_val_precision', 0.0)\n\
          \        recall = training_metrics.get('final_val_recall', 0.0)\n      \
          \  f1_score = 2 * (precision * recall) / (precision + recall) if (precision\
          \ + recall) > 0 else 0.0\n\n        return (accuracy, f1_score)\n    else:\n\
          \        return (0.0, 0.0)\n\n"
        image: entity-matcher:latest
    exec-model-validation-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_validation_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas'\
          \ 'kfp==2.0.1' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_validation_component(\n    test_accuracy: float,\n    test_f1:\
          \ float,\n    min_accuracy: float = 0.7,\n    min_f1: float = 0.7\n) ->\
          \ str:\n    \"\"\"Composant de validation du mod\xE8le\"\"\"\n\n    print(f\"\
          Validation - Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n    print(f\"\
          Seuils - Min Accuracy: {min_accuracy}, Min F1: {min_f1}\")\n\n    if test_accuracy\
          \ >= min_accuracy and test_f1 >= min_f1:\n        status = \"APPROVED\"\n\
          \        message = f\"\u2705 Mod\xE8le valid\xE9 - Accuracy: {test_accuracy:.4f},\
          \ F1: {test_f1:.4f}\"\n    else:\n        status = \"REJECTED\"\n      \
          \  message = f\"\u274C Mod\xE8le rejet\xE9 - Accuracy: {test_accuracy:.4f}\
          \ (min: {min_accuracy}), F1: {test_f1:.4f} (min: {min_f1})\"\n\n    print(message)\n\
          \    return status\n\n"
        image: python:3.9-slim
pipelineInfo:
  description: "Pipeline de re-entra\xEEnement avec nouvelles donn\xE9es"
  name: entity-matching-retrain-pipeline
root:
  dag:
    outputs:
      artifacts:
        data-preprocessing-component-output_metrics:
          artifactSelectors:
          - outputArtifactKey: output_metrics
            producerSubtask: data-preprocessing-component
        model-evaluation-component-output_metrics:
          artifactSelectors:
          - outputArtifactKey: output_metrics
            producerSubtask: model-evaluation-component
        model-training-component-output_metrics:
          artifactSelectors:
          - outputArtifactKey: output_metrics
            producerSubtask: model-training-component
    tasks:
      data-preprocessing-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-preprocessing-component
        taskInfo:
          name: data-preprocessing-component
      model-evaluation-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-evaluation-component
        dependentTasks:
        - data-preprocessing-component
        - model-training-component
        inputs:
          artifacts:
            input_dataset:
              taskOutputArtifact:
                outputArtifactKey: output_dataset
                producerTask: data-preprocessing-component
            input_model:
              taskOutputArtifact:
                outputArtifactKey: output_model
                producerTask: model-training-component
          parameters:
            accuracy:
              taskOutputParameter:
                outputParameterKey: accuracy
                producerTask: model-training-component
            f1_score:
              taskOutputParameter:
                outputParameterKey: f1_score
                producerTask: model-training-component
        taskInfo:
          name: model-evaluation-component
      model-training-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-training-component
        dependentTasks:
        - data-preprocessing-component
        inputs:
          artifacts:
            input_dataset:
              taskOutputArtifact:
                outputArtifactKey: output_dataset
                producerTask: data-preprocessing-component
          parameters:
            batch_size:
              componentInputParameter: batch_size
            epochs:
              componentInputParameter: epochs
            learning_rate:
              componentInputParameter: learning_rate
            num_positives:
              taskOutputParameter:
                outputParameterKey: num_positives
                producerTask: data-preprocessing-component
            num_samples:
              taskOutputParameter:
                outputParameterKey: num_samples
                producerTask: data-preprocessing-component
        taskInfo:
          name: model-training-component
      model-validation-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-validation-component
        dependentTasks:
        - model-evaluation-component
        inputs:
          parameters:
            min_accuracy:
              componentInputParameter: min_accuracy
            min_f1:
              componentInputParameter: min_f1
            test_accuracy:
              taskOutputParameter:
                outputParameterKey: test_accuracy
                producerTask: model-evaluation-component
            test_f1:
              taskOutputParameter:
                outputParameterKey: test_f1
                producerTask: model-evaluation-component
        taskInfo:
          name: model-validation-component
  inputDefinitions:
    parameters:
      base_model_path:
        parameterType: STRING
      batch_size:
        defaultValue: 32.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      epochs:
        defaultValue: 5.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      learning_rate:
        defaultValue: 0.0001
        isOptional: true
        parameterType: NUMBER_DOUBLE
      min_accuracy:
        defaultValue: 0.75
        isOptional: true
        parameterType: NUMBER_DOUBLE
      min_f1:
        defaultValue: 0.75
        isOptional: true
        parameterType: NUMBER_DOUBLE
  outputDefinitions:
    artifacts:
      data-preprocessing-component-output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-evaluation-component-output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-training-component-output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
