import os
import json
import pandas as pd
import numpy as np
from datetime import datetime

def check_data_files():
    """VÃ©rifie la prÃ©sence des fichiers de donnÃ©es requis"""
    required_files = [
        "data/source_final.csv",
        "data/reference_final.csv"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print("âŒ Fichiers manquants:")
        for file in missing_files:
            print(f"  - {file}")
        return False
    else:
        print("âœ… Tous les fichiers de donnÃ©es sont prÃ©sents")
        return True

def create_directories():
    """CrÃ©e les rÃ©pertoires nÃ©cessaires"""
    directories = ["models", "data", "logs", "outputs"]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"ğŸ“ RÃ©pertoire crÃ©Ã©/vÃ©rifiÃ©: {directory}")

def log_experiment(experiment_name, metrics, config=None):
    """Enregistre les rÃ©sultats d'une expÃ©rience"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    experiment_data = {
        "experiment_name": experiment_name,
        "timestamp": timestamp,
        "metrics": metrics
    }
    
    if config:
        experiment_data["config"] = config
    
    # CrÃ©er le fichier de log
    log_file = f"logs/experiment_{experiment_name}_{timestamp}.json"
    
    with open(log_file, 'w') as f:
        json.dump(experiment_data, f, indent=2)
    
    print(f"ğŸ“ ExpÃ©rience enregistrÃ©e: {log_file}")

def get_dataset_info():
    """Retourne des informations sur le dataset"""
    try:
        if os.path.exists("models/processed_dataset.csv"):
            df = pd.read_csv("models/processed_dataset.csv")
            
            info = {
                "total_samples": len(df),
                "positive_samples": sum(df['label']),
                "negative_samples": len(df) - sum(df['label']),
                "balance_ratio": sum(df['label']) / len(df),
                "columns": list(df.columns)
            }
            
            print("ğŸ“Š Informations du dataset:")
            print(f"  - Total: {info['total_samples']} Ã©chantillons")
            print(f"  - Positifs: {info['positive_samples']} ({info['balance_ratio']:.2%})")
            print(f"  - NÃ©gatifs: {info['negative_samples']} ({1-info['balance_ratio']:.2%})")
            
            return info
        else:
            print("âŒ Dataset traitÃ© non trouvÃ©")
            return None
            
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture du dataset: {e}")
        return None

def validate_preprocessing_outputs():
    """Valide les sorties du preprocessing"""
    required_files = [
        "models/X1_train.npy",
        "models/X2_train.npy", 
        "models/y_train.npy",
        "models/X1_test.npy",
        "models/X2_test.npy",
        "models/y_test.npy",
        "models/tokenizer.pkl",
        "models/processed_dataset.csv"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print("âŒ Fichiers de preprocessing manquants:")
        for file in missing_files:
            print(f"  - {file}")
        return False
    else:
        print("âœ… Tous les fichiers de preprocessing sont prÃ©sents")
        
        # VÃ©rifier la cohÃ©rence des dimensions
        try:
            X1_train = np.load("models/X1_train.npy")
            X2_train = np.load("models/X2_train.npy")
            y_train = np.load("models/y_train.npy")
            
            print(f"ğŸ“Š Dimensions d'entraÃ®nement:")
            print(f"  - X1_train: {X1_train.shape}")
            print(f"  - X2_train: {X2_train.shape}")
            print(f"  - y_train: {y_train.shape}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Erreur lors de la validation: {e}")
            return False

def validate_model_outputs():
    """Valide les sorties de l'entraÃ®nement"""
    model_files = [
        "models/siamese_entity_matcher.keras",
        "models/training_metrics.json"
    ]
    
    missing_files = []
    for file_path in model_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print("âŒ Fichiers de modÃ¨le manquants:")
        for file in missing_files:
            print(f"  - {file}")
        return False
    else:
        print("âœ… Tous les fichiers de modÃ¨le sont prÃ©sents")
        
        # Lire les mÃ©triques d'entraÃ®nement
        try:
            with open("models/training_metrics.json", 'r') as f:
                metrics = json.load(f)
            
            print("ğŸ“Š MÃ©triques d'entraÃ®nement finales:")
            for key, value in metrics.items():
                print(f"  - {key}: {value:.4f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Erreur lors de la lecture des mÃ©triques: {e}")
            return False

def cleanup_temp_files():
    """Nettoie les fichiers temporaires"""
    temp_patterns = [
        "*.tmp",
        "*.log",
        "checkpoint*",
        "__pycache__",
    ]
    
    cleaned_files = 0
    for pattern in temp_patterns:
        import glob
        files = glob.glob(pattern, recursive=True)
        for file in files:
            try:
                if os.path.isfile(file):
                    os.remove(file)
                    cleaned_files += 1
                elif os.path.isdir(file):
                    import shutil
                    shutil.rmtree(file)
                    cleaned_files += 1
            except Exception as e:
                print(f"âš ï¸ Impossible de supprimer {file}: {e}")
    
    if cleaned_files > 0:
        print(f"ğŸ§¹ {cleaned_files} fichiers temporaires nettoyÃ©s")
    else:
        print("âœ… Aucun fichier temporaire Ã  nettoyer")

def generate_project_report():
    """GÃ©nÃ¨re un rapport complet du projet"""
    report = {
        "project_status": {},
        "dataset_info": {},
        "model_info": {},
        "generated_at": datetime.now().isoformat()
    }
    
    # Statut des fichiers
    report["project_status"]["data_files_ok"] = check_data_files()
    report["project_status"]["preprocessing_ok"] = validate_preprocessing_outputs()
    report["project_status"]["model_ok"] = validate_model_outputs()
    
    # Informations dataset
    dataset_info = get_dataset_info()
    if dataset_info:
        report["dataset_info"] = dataset_info
    
    # Informations modÃ¨le
    if os.path.exists("models/training_metrics.json"):
        with open("models/training_metrics.json", 'r') as f:
            report["model_info"]["training_metrics"] = json.load(f)
    
    if os.path.exists("models/evaluation_results.json"):
        with open("models/evaluation_results.json", 'r') as f:
            report["model_info"]["evaluation_results"] = json.load(f)
    
    # Sauvegarder le rapport
    report_file = f"outputs/project_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"ğŸ“‹ Rapport projet gÃ©nÃ©rÃ©: {report_file}")
    
    # Affichage rÃ©sumÃ©
    print("\n" + "="*50)
    print("ğŸ“Š RÃ‰SUMÃ‰ DU PROJET")
    print("="*50)
    print(f"âœ… Fichiers de donnÃ©es: {'OK' if report['project_status']['data_files_ok'] else 'MANQUANT'}")
    print(f"âœ… Preprocessing: {'OK' if report['project_status']['preprocessing_ok'] else 'MANQUANT'}")
    print(f"âœ… ModÃ¨le: {'OK' if report['project_status']['model_ok'] else 'MANQUANT'}")
    
    if dataset_info:
        print(f"ğŸ“Š Dataset: {dataset_info['total_samples']} Ã©chantillons")
    
    return report

def check_dependencies():
    """VÃ©rifie les dÃ©pendances Python"""
    required_packages = [
        'pandas', 'numpy', 'tensorflow', 'scikit-learn', 
        'rapidfuzz', 'matplotlib', 'seaborn'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ Packages Python manquants:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\nInstallez avec: pip install " + " ".join(missing_packages))
        return False
    else:
        print("âœ… Toutes les dÃ©pendances Python sont installÃ©es")
        return True

def main():
    """Fonction principale des utilitaires"""
    print("ğŸ› ï¸ Utilitaires du projet Entity Matching")
    print("="*50)
    
    # CrÃ©er les rÃ©pertoires
    create_directories()
    
    # VÃ©rifier les dÃ©pendances
    check_dependencies()
    
    # GÃ©nÃ©rer le rapport
    generate_project_report()
    
    print("\nâœ… VÃ©rifications terminÃ©es!")

if __name__ == "__main__":
    main()

    